# -*- coding: utf-8 -*-
"""Feature_Decorrelation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XVU3GxgVXmKaYlRRQqXdeejyX8I9wj70

Feature Decorrelation \\
Authors: Davide Perozzi, Fabian Vincenzi

# Imports & functions
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/giuluck/causalgen.git
# %cd causalgen

seeds = [42 * i for i in range(1, 26)]
num_data = 10000
epochs = 2

import os
import random
import numpy as np
import torch
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from causalgen.generator import Generator
import matplotlib.pyplot as plt
from matplotlib.ticker import StrMethodFormatter
import seaborn as sns
from tqdm import tqdm
from keras.models import Sequential
from sklearn.tree import DecisionTreeRegressor
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

def set_reproducibility(seed):
  random.seed(seed)
  np.random.seed(seed)
  rng = np.random.default_rng(seed)
  torch.manual_seed(seed)
  tf.random.set_seed(1)
  os.environ['TF_DETERMINISTIC_OPS'] = '1'

# Compute the mean of the arrays and L1 and L2 norms of each array with respect to the mean array

def print_L1_L2(C_import, C_prime_import):
  mean_array = np.mean(C_import, axis=0)
  l1_norm = np.linalg.norm(C_import - mean_array, ord=1, axis=1)
  l2_norm = np.linalg.norm(C_import - mean_array, ord=2, axis=1)

  print("model C")
  print("L1 norm")
  print("mean:", np.mean(l1_norm))
  print("L2 norm")
  print("mean:", np.mean(l2_norm))
  print("\n")

  mean_array = np.mean(C_prime_import, axis=0)
  l1_norm_prime = np.linalg.norm(C_prime_import - mean_array, ord=1, axis=1)
  l2_norm_prime = np.linalg.norm(C_prime_import - mean_array, ord=2, axis=1)

  print("model C_prime")
  print("L1 norm")
  print("mean:", np.mean(l1_norm_prime))
  print("L2 norm")
  print("mean:", np.mean(l2_norm_prime))
  print("\n")

def plot_coefs(results):
  plt.figure(figsize=(5, 3))

  sns.set(style="whitegrid")
  ax = sns.barplot(data=results, estimator='mean', errorbar='sd')
  ax.set_ylabel('Values')
  ax.set_xlabel('Features')

  plt.show()

"""# Graph #1"""

seed = 42

set_reproducibility(seed)

dg = Generator(seed=seed)

A = dg.normal(mu=0, sigma=1, name='A')

B = dg.normal(mu=0, sigma=1, name='B')

C = dg.descendant(A + B, name='C')

D = dg.descendant(A + B - 3 * C, name='D')

df = dg.generate(hidden=True)
dg.visualize()

"""## Generate data"""

print('\n\nGenerated Dataset')
df = pd.DataFrame(dg.generate(num=num_data))
df.head()

"""## Capture correlation

Capture correlation [A->C, B->C], train the model over A and B to predict C

Then compute C_prime as C-M(A,B)
"""

def build_C_prime(df):

  model = keras.Sequential([
      keras.layers.Dense(10, input_shape=[2], activation='relu'),
      keras.layers.Dense(1)
      ])

  model.compile(loss='mse', optimizer='adam')
  model.fit(df[['A','B']], df['C'],
      epochs=10,
      batch_size=32,
      verbose=False
      )

  df['C_new'] = model.predict(df[['A','B']])
  df['C_prime'] = df['C'] - df['C_new']

  return df

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

df = build_C_prime(df)

df.head()

"""## Stochastic Linear Regression

### Training C
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_import_nn = []
loss_history_C = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = Sequential()
  model_C.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C.fit(df[['A','B','C']], df['D'], epochs=epochs)

  y_pred = model_C.predict(df[['A','B','C']])

  loss_history_C.append(r2_score(df['D'], y_pred))
  C_import_nn.append(model_C.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_import_nn, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Training C_prime"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_prime_import_nn = []
loss_history_prime = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = Sequential()
  model_C_prime.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C_prime.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C_prime.fit(df[['A','B','C_prime']], df['D'], epochs=epochs)

  y_pred = model_C_prime.predict(df[['A','B','C_prime']])

  loss_history_prime.append(r2_score(df['D'], y_pred))
  C_prime_import_nn.append(model_C_prime.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_prime_import_nn, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_nn, C_prime_import_nn)

print("\n")
print("R2 C: ", np.mean(loss_history_C))
print("R2 C_prime: ", np.mean(loss_history_prime))

"""## Stochastic Decision Tree

### Train using C
"""

from sklearn.tree import DecisionTreeRegressor

C_import_tree = []
loss_history_C_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C.fit(df[['A','B','C']], df['D'])

  loss_history_C_tree.append(model_C.score(df[['A','B','C']], df['D']))
  C_import_tree.append(model_C.feature_importances_)

results = pd.DataFrame(C_import_tree, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Train using C_prime"""

from sklearn.tree import DecisionTreeRegressor

C_prime_import_tree = []
loss_history_prime_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C_prime.fit(df[['A','B','C_prime']], df['D'])

  loss_history_prime_tree.append(model_C_prime.score(df[['A','B','C_prime']], df['D']))
  C_prime_import_tree.append(model_C_prime.feature_importances_)

results = pd.DataFrame(C_prime_import_tree, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_tree, C_prime_import_tree)

print("\n")
print("R2 C: ", np.mean(loss_history_C_tree))
print("R2 C_prime: ", np.mean(loss_history_prime_tree))

"""# Graph #2"""

seed = 42

set_reproducibility(seed)

dg = Generator(seed=seed)

A = dg.normal(mu=0.5, sigma=0.5, name='A')

B = dg.normal(mu=0.5, sigma=0.5, name='B')

C = dg.descendant(A + B, name='C')

D = dg.descendant(A + B - 3 * C, name='D')

df = dg.generate(hidden=True)
dg.visualize()

"""## Generate data"""

print('\n\nGenerated Dataset')
df = pd.DataFrame(dg.generate(num=num_data))
df.head()

"""## Capture correlation

Capture correlation [A->C, B->C], train the model over A and B to predict C

Then compute C_prime as C-M(A,B)
"""

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

def build_C_prime(df):

  model = keras.Sequential([
      keras.layers.Dense(10, input_shape=[2], activation='relu'),
      keras.layers.Dense(1)
      ])

  model.compile(loss='mse', optimizer='adam')
  model.fit(df[['A','B']], df['C'],
      epochs=10,
      batch_size=32,
      verbose=False
      )

  df['C_new'] = model.predict(df[['A','B']])
  df['C_prime'] = df['C'] - df['C_new']

  return df

df = build_C_prime(df)

df.head()

"""## Stochastic Linear Regression

### Training C
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_import_nn = []
loss_history_C = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = Sequential()
  model_C.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C.fit(df[['A','B','C']], df['D'], epochs=epochs)

  y_pred = model_C.predict(df[['A','B','C']])

  loss_history_C.append(r2_score(df['D'], y_pred))
  C_import_nn.append(model_C.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_import_nn, index=seeds, columns=["A", "B","C"])

plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Training C_prime"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_prime_import_nn = []
loss_history_prime = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = Sequential()
  model_C_prime.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C_prime.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C_prime.fit(df[['A','B','C_prime']], df['D'], epochs=epochs)

  y_pred = model_C_prime.predict(df[['A','B','C_prime']])

  loss_history_prime.append(r2_score(df['D'], y_pred))
  C_prime_import_nn.append(model_C_prime.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_prime_import_nn, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_nn, C_prime_import_nn)

print("\n")
print("R2 C: ", np.mean(loss_history_C))
print("R2 C_prime: ", np.mean(loss_history_prime))

"""## Stochastic Decision Tree

### Train using C
"""

from sklearn.tree import DecisionTreeRegressor

C_import_tree = []
loss_history_C_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C.fit(df[['A','B','C']], df['D'])

  loss_history_C_tree.append(model_C.score(df[['A','B','C']], df['D']))
  C_import_tree.append(model_C.feature_importances_)

results = pd.DataFrame(C_import_tree, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Train using C_prime"""

from sklearn.tree import DecisionTreeRegressor

C_prime_import_tree = []
loss_history_prime_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C_prime.fit(df[['A','B','C_prime']], df['D'])

  loss_history_prime_tree.append(model_C_prime.score(df[['A','B','C_prime']], df['D']))
  C_prime_import_tree.append(model_C_prime.feature_importances_)

results = pd.DataFrame(C_prime_import_tree, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_tree, C_prime_import_tree)

print("\n")
print("R2 C: ", np.mean(loss_history_C_tree))
print("R2 C_prime: ", np.mean(loss_history_prime_tree))

"""# Graph #3"""

seed = 42
set_reproducibility(seed)

dg = Generator(seed=seed)

A = dg.normal(mu=1.0, sigma=0.2, name='A')
B = dg.normal(mu=0.8, sigma=0.4, name='B')

C = dg.descendant(A + B, name='C')
D = dg.descendant(A + B - 3 * C, name='D')

df = dg.generate(hidden=True)
dg.visualize()

"""## Generate data"""

print('\n\nGenerated Dataset')
df = pd.DataFrame(dg.generate(num=num_data))
df.head()

"""## Capture correlation

Capture correlation [A->C, B->C], train the model over A and B to predict C

Then compute C_prime as C-M(A,B)
"""

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

def build_C_prime(df):

  model = keras.Sequential([
      keras.layers.Dense(10, input_shape=[2], activation='relu'),
      keras.layers.Dense(1)
      ])

  model.compile(loss='mse', optimizer='adam')
  model.fit(df[['A','B']], df['C'],
      epochs=10,
      batch_size=32,
      verbose=False
      )

  df['C_new'] = model.predict(df[['A','B']])
  df['C_prime'] = df['C'] - df['C_new']

  return df

df = build_C_prime(df)

df.head()

"""## Stochastic Linear Regression

### Training C
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_import_nn = []
loss_history_C = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = Sequential()
  model_C.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C.fit(df[['A','B','C']], df['D'], epochs=epochs)

  y_pred = model_C.predict(df[['A','B','C']])

  loss_history_C.append(r2_score(df['D'], y_pred))
  C_import_nn.append(model_C.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_import_nn, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Training C_prime"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_prime_import_nn = []
loss_history_prime = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = Sequential()
  model_C_prime.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C_prime.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C_prime.fit(df[['A','B','C_prime']], df['D'], epochs=epochs)

  y_pred = model_C_prime.predict(df[['A','B','C_prime']])

  loss_history_prime.append(r2_score(df['D'], y_pred))
  C_prime_import_nn.append(model_C_prime.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_prime_import_nn, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_nn, C_prime_import_nn)

print("\n")
print("R2 C: ", np.mean(loss_history_C))
print("R2 C_prime: ", np.mean(loss_history_prime))

"""## Stochastic Decision Tree

### Train using C
"""

from sklearn.tree import DecisionTreeRegressor

C_import_tree = []
loss_history_C_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C.fit(df[['A','B','C']], df['D'])

  loss_history_C_tree.append(model_C.score(df[['A','B','C']], df['D']))
  C_import_tree.append(model_C.feature_importances_)

results = pd.DataFrame(C_import_tree, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Train using C_prime"""

from sklearn.tree import DecisionTreeRegressor

C_prime_import_tree = []
loss_history_prime_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C_prime.fit(df[['A','B','C_prime']], df['D'])

  loss_history_prime_tree.append(model_C_prime.score(df[['A','B','C_prime']], df['D']))
  C_prime_import_tree.append(model_C_prime.feature_importances_)

results = pd.DataFrame(C_prime_import_tree, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_tree, C_prime_import_tree)

print("\n")
print("R2 C: ", np.mean(loss_history_C_tree))
print("R2 C_prime: ", np.mean(loss_history_prime_tree))

"""# Graph with noise"""

seed = 42

set_reproducibility(seed)

dg = Generator(seed=seed)

A = dg.normal(mu=0, sigma=1, name='A')

B = dg.normal(mu=0, sigma=1, name='B')

C = dg.descendant(A + B, name='C')

D = dg.descendant(A + B - 3 * C + dg.noise(), name='D')

df = dg.generate(hidden=True)
dg.visualize()

"""## Generate data"""

print('\n\nGenerated Dataset')
df = pd.DataFrame(dg.generate(num=num_data))
df.head()

"""## Capture correlation

Capture correlation [A->C, B->C], train the model over A and B to predict C

Then compute C_prime as C-M(A,B)
"""

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

def build_C_prime(df):

  model = keras.Sequential([
      keras.layers.Dense(10, input_shape=[2], activation='relu'),
      keras.layers.Dense(1)
      ])

  model.compile(loss='mse', optimizer='adam')
  model.fit(df[['A','B']], df['C'],
      epochs=10,
      batch_size=32,
      verbose=False
      )

  df['C_new'] = model.predict(df[['A','B']])
  df['C_prime'] = df['C'] - df['C_new']

  return df

df = build_C_prime(df)

df.head()

"""## Stochastic Linear Regression

### Training C
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_import_nn = []
loss_history_C = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = Sequential()
  model_C.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C.fit(df[['A','B','C']], df['D'], epochs=epochs)
  y_pred = model_C.predict(df[['A','B','C']])

  loss_history_C.append(r2_score(df['D'], y_pred))
  C_import_nn.append(model_C.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_import_nn, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Training C_prime"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_prime_import_nn = []
loss_history_prime = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = Sequential()
  model_C_prime.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C_prime.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C_prime.fit(df[['A','B','C_prime']], df['D'], epochs=epochs)

  y_pred = model_C_prime.predict(df[['A','B','C_prime']])

  loss_history_prime.append(r2_score(df['D'], y_pred))
  C_prime_import_nn.append(model_C_prime.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_prime_import_nn, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_nn, C_prime_import_nn)

print("\n")
print("R2 C: ", np.mean(loss_history_C))
print("R2 C_prime: ", np.mean(loss_history_prime))

"""## Stochastic Decision Tree

### Train using C
"""

from sklearn.tree import DecisionTreeRegressor

C_import_tree = []
loss_history_C_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C.fit(df[['A','B','C']], df['D'])

  loss_history_C_tree.append(model_C.score(df[['A','B','C']], df['D']))
  C_import_tree.append(model_C.feature_importances_)

results = pd.DataFrame(C_import_tree, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Train using C_prime"""

from sklearn.tree import DecisionTreeRegressor

C_prime_import_tree = []
loss_history_prime_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C_prime.fit(df[['A','B','C_prime']], df['D'])

  loss_history_prime_tree.append(model_C_prime.score(df[['A','B','C_prime']], df['D']))
  C_prime_import_tree.append(model_C_prime.feature_importances_)

results = pd.DataFrame(C_prime_import_tree, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_tree, C_prime_import_tree)

print("\n")
print("R2 C: ", np.mean(loss_history_C_tree))
print("R2 C_prime: ", np.mean(loss_history_prime_tree))

"""# Graph with noise #2"""

seed = 42

set_reproducibility(seed)

dg = Generator(seed=seed)

A = dg.normal(mu=0.5, sigma=0.5, name='A')

B = dg.normal(mu=0.5, sigma=0.5, name='B')

C = dg.descendant(A + B, name='C')

D = dg.descendant(A + B - 3 * C + dg.noise(), name='D')

df = dg.generate(hidden=True)
dg.visualize()

"""## Generate data"""

print('\n\nGenerated Dataset')
df = pd.DataFrame(dg.generate(num=num_data))
df.head()

"""## Capture correlation

Capture correlation [A->C, B->C], train the model over A and B to predict C

Then compute C_prime as C-M(A,B)
"""

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

def build_C_prime(df):

  model = keras.Sequential([
      keras.layers.Dense(10, input_shape=[2], activation='relu'),
      keras.layers.Dense(1)
      ])

  model.compile(loss='mse', optimizer='adam')
  model.fit(df[['A','B']], df['C'],
      epochs=10,
      batch_size=32,
      verbose=False
      )

  df['C_new'] = model.predict(df[['A','B']])
  df['C_prime'] = df['C'] - df['C_new']

  return df

df = build_C_prime(df)

df.head()

"""## Stochastic Linear Regression

### Training C
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_import_nn = []
loss_history_C = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = Sequential()
  model_C.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C.fit(df[['A','B','C']], df['D'], epochs=epochs)

  y_pred = model_C.predict(df[['A','B','C']])

  loss_history_C.append(r2_score(df['D'], y_pred))
  C_import_nn.append(model_C.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_import_nn, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Training C_prime"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.metrics import r2_score

C_prime_import_nn = []
loss_history_prime = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = Sequential()
  model_C_prime.add(Dense(1, input_dim=3, activation='linear'))

  sgd = SGD()
  model_C_prime.compile(loss='mean_squared_error', optimizer=sgd)

  # fit the model and store the weights
  model_C_prime.fit(df[['A','B','C_prime']], df['D'], epochs=epochs)

  y_pred = model_C_prime.predict(df[['A','B','C_prime']])

  loss_history_prime.append(r2_score(df['D'], y_pred))
  C_prime_import_nn.append(model_C_prime.layers[0].get_weights()[0].flatten())

results = pd.DataFrame(C_prime_import_nn, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_nn, C_prime_import_nn)

print("\n")
print("R2 C: ", np.mean(loss_history_C))
print("R2 C_prime: ", np.mean(loss_history_prime))

"""## Stochastic Decision Tree

### Train using C
"""

from sklearn.tree import DecisionTreeRegressor

C_import_tree = []
loss_history_C_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C.fit(df[['A','B','C']], df['D'])

  loss_history_C_tree.append(model_C.score(df[['A','B','C']], df['D']))
  C_import_tree.append(model_C.feature_importances_)

results = pd.DataFrame(C_import_tree, index=seeds, columns=["A", "B","C"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_C_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Train using C_prime"""

from sklearn.tree import DecisionTreeRegressor

C_prime_import_tree = []
loss_history_prime_tree = []

# train the model multiple times with different seeds
for seed in seeds:
  set_reproducibility(seed)

  model_C_prime = DecisionTreeRegressor(splitter="random", random_state=seed)

  model_C_prime.fit(df[['A','B','C_prime']], df['D'])

  loss_history_prime_tree.append(model_C_prime.score(df[['A','B','C_prime']], df['D']))
  C_prime_import_tree.append(model_C_prime.feature_importances_)

results = pd.DataFrame(C_prime_import_tree, index=seeds, columns=["A", "B","C_prime"])
plot_coefs(results)

seeds_ticks = range(len(seeds))
plt.plot(seeds_ticks, loss_history_prime_tree)
plt.xticks(seeds_ticks, seeds, rotation=90)
plt.show()

"""### Compare trainings"""

print_L1_L2(C_import_tree, C_prime_import_tree)

print("\n")
print("R2 C: ", np.mean(loss_history_C_tree))
print("R2 C_prime: ", np.mean(loss_history_prime_tree))

"""### Functions"""

fixed_seed = 69

def compute_C_prime(df):

  model = keras.Sequential([
      keras.layers.Dense(10, input_shape=[2], activation='relu'),
      keras.layers.Dense(1)
      ])

  model.compile(loss='mse', optimizer='adam')
  model.fit(df[['A','B']], df['C'],
      epochs=10,
      batch_size=32,
      verbose=0
      )

  df['C_new'] = model.predict(df[['A','B']], verbose=0)
  df['C_prime'] = df['C'] - df['C_new']

  return df

def build_SLR():
  model = Sequential()
  model.add(Dense(1, input_dim=3, activation='linear'))
  sgd = SGD()
  model.compile(loss='mean_squared_error', optimizer=sgd)
  return model

def build_SDT(seed):
  model = DecisionTreeRegressor(splitter="random", random_state=seed)
  return model

def fit(model, dataset, epochs=None, C_prime=False):
    if C_prime:
        feature_columns = ['A', 'B', 'C_prime']
    else:
        feature_columns = ['A', 'B', 'C']

    if isinstance(model, DecisionTreeRegressor):
        model.fit(dataset[feature_columns], dataset['D'])
        y_pred = model.predict(dataset[feature_columns])
        score = r2_score(dataset['D'], y_pred)
        weights = model.feature_importances_
    else:
        if epochs is None:
            raise ValueError("Epochs parameter must be provided for Sequential models.")
        model.fit(dataset[feature_columns], dataset['D'], epochs=epochs, verbose=0)
        y_pred = model.predict(dataset[feature_columns], verbose=0)
        score = r2_score(dataset['D'], y_pred)
        weights = model.layers[0].get_weights()[0].flatten()

    return weights, score

def run_experiment(generator, epochs, fixed_seed, seeds, C_prime=False):

  slr_weights = []
  slr_scores_history = []
  sdt_weights = []
  sdt_scores_history = []

  for seed in tqdm(seeds, desc="Running experiment", unit="seed"):

    # Build dataset
    set_reproducibility(seed)
    df = pd.DataFrame(generator(seed, num_data))
    if C_prime:
      df = compute_C_prime(df)

    # set fixed seed, build models and train them
    set_reproducibility(fixed_seed)
    slr = build_SLR()
    sdt = build_SDT(fixed_seed)

    weights, scores = fit(slr, df, epochs=epochs, C_prime=C_prime)
    slr_weights.append(weights)
    slr_scores_history.append(scores)

    weights, scores = fit(sdt, df, C_prime=C_prime)
    sdt_weights.append(weights)
    sdt_scores_history.append(scores)

  return slr_weights, slr_scores_history, sdt_weights, sdt_scores_history

def print_bold(text):
    bold_text = f"\033[1m{text}\033[0m"
    print(bold_text)

"""## Graph"""

def generate_data(seed, num_data):

  dg = Generator(seed=seed)

  A = dg.normal(mu=0, sigma=1, name='A')

  B = dg.normal(mu=0, sigma=1, name='B')

  C = dg.descendant(A + B, name='C')

  D = dg.descendant(A + B - 3 * C, name='D')

  dg.generate(hidden=True)

  return dg.generate(num=num_data)

"""### Train with C"""

slr_weights, slr_scores_history, sdt_weights, sdt_scores_history = run_experiment(generate_data, epochs, fixed_seed, seeds)

print_bold("Stochastic Linear Regressor weights variance with C")
plot_coefs( pd.DataFrame(slr_weights, columns=["A", "B","C"]) )
print("\n")
print_bold("Stochastic Decision Tree weights variance with C")
plot_coefs( pd.DataFrame(sdt_weights, columns=["A", "B","C"]) )

"""### Train with C_prime"""

slr_weights_C_prime, slr_score_history_C_prime, sdt_weights_C_prime, sdt_score_history_C_prime = run_experiment(generate_data, epochs, fixed_seed, seeds, C_prime=True)

print_bold("Stochastic Linear Regressor weights variance with C_prime")
plot_coefs( pd.DataFrame(slr_weights_C_prime, columns=["A", "B","C_prime"]) )
print("\n")
print_bold("Stochastic Decision Tree weights variance with C_prime")
plot_coefs( pd.DataFrame(sdt_weights_C_prime, columns=["A", "B","C_prime"]) )

print_L1_L2(slr_weights, slr_weights_C_prime)
print_L1_L2(sdt_weights, sdt_weights_C_prime)

"""## Graph with noise"""

def generate_data_w_noise(seed, num_data):

  dg = Generator(seed=seed)

  A = dg.normal(mu=0, sigma=1, name='A')

  B = dg.normal(mu=0, sigma=1, name='B')

  C = dg.descendant(A + B, name='C')

  D = dg.descendant(A + B - 3 * C, noise=0.2, name='D')

  dg.generate(hidden=True)

  return dg.generate(num=num_data)

"""### Train with C"""

slr_weights, slr_scores_history, sdt_weights, sdt_scores_history = run_experiment(generate_data_w_noise, epochs, fixed_seed, seeds)

print_bold("Stochastic Linear Regressor weights variance with C")
plot_coefs( pd.DataFrame(slr_weights, columns=["A", "B","C"]) )
print("\n")
print_bold("Stochastic Decision Tree weights variance with C")
plot_coefs( pd.DataFrame(sdt_weights, columns=["A", "B","C"]) )

"""### Train with C_prime"""

slr_weights_C_prime, slr_score_history_C_prime, sdt_weights_C_prime, sdt_score_history_C_prime = run_experiment(generate_data_w_noise, epochs, fixed_seed, seeds, C_prime=True)

print_bold("Stochastic Linear Regressor weights variance with C_prime")
plot_coefs( pd.DataFrame(slr_weights_C_prime, columns=["A", "B","C_prime"]) )
print("\n")
print_bold("Stochastic Decision Tree weights variance with C_prime")
plot_coefs( pd.DataFrame(sdt_weights_C_prime, columns=["A", "B","C_prime"]) )

print_L1_L2(slr_weights, slr_weights_C_prime)
print_L1_L2(sdt_weights, sdt_weights_C_prime)